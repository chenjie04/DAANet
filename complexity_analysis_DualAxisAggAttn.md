# DualAxisAggAttn æ¨¡å—å¤æ‚åº¦åˆ†æž

ç»™å®šè¾“å…¥å½¢çŠ¶ä¸º [B, C, H, W]ï¼Œè¦æ±‚è¾“å‡ºå½¢çŠ¶åŒæ ·ä¸º [B, C, H, W]ï¼Œè¯·è¯¦ç»†åˆ†æž DualAxisAggAttn ç±»çš„ç©ºé—´å¤æ‚åº¦å’Œæ—¶é—´å¤æ‚åº¦ã€‚

```python

import torch
import torch.nn as nn
import torch.nn.functional as F


def autopad(k, p=None, d=1):  # kernel, padding, dilation
    """Pad to 'same' shape outputs."""
    if d > 1:
        k = (
            d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]
        )  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


class Conv(nn.Module):
    """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""

    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        """Initialize Conv layer with given arguments including activation."""
        super().__init__()
        self.conv = nn.Conv2d(
            c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False
        )
        self.bn = nn.BatchNorm2d(c2)
        self.act = (
            self.default_act
            if act is True
            else act if isinstance(act, nn.Module) else nn.Identity()
        )

    def forward(self, x):
        """Apply convolution, batch normalization and activation to input tensor."""
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """Apply convolution and activation without batch normalization."""
        return self.act(self.conv(x))


class light_ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv_block = nn.Sequential(
            Conv(c1=in_channels, c2=in_channels, k=1, act=True),
            Conv(c1=in_channels, c2=in_channels, k=3, g=in_channels, act=True),
            Conv(c1=in_channels, c2=out_channels, k=1, act=True),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.conv_block(x)

class DualAxisAggAttn(nn.Module):
    def __init__(
        self,
        channels: int,
        middle_ratio: float = 0.5,
    ):
        super().__init__()
        self.channels = channels
        middle_channels = int(channels * middle_ratio)
        self.middle_channels = middle_channels

        self.main_conv = Conv(c1=channels, c2=middle_channels, k=1, act=True)
        self.short_conv = Conv(c1=channels, c2=middle_channels, k=1, act=True)

        self.qkv = nn.ModuleDict(
            {
                "W": nn.Conv2d(
                    in_channels=middle_channels,
                    out_channels=1 + 2 * middle_channels,
                    kernel_size=1,
                    bias=True,
                ),
                "H": nn.Conv2d(
                    in_channels=middle_channels,
                    out_channels=1 + 2 * middle_channels,
                    kernel_size=1,
                    bias=True,
                ),
            }
        )

        self.conv_fusion = nn.ModuleDict(
            {
                "W": light_ConvBlock(
                    in_channels=middle_channels, out_channels=middle_channels
                ),
                "H": light_ConvBlock(
                    in_channels=middle_channels, out_channels=middle_channels
                ),
            }
        )


        final_channels = int(2 * middle_channels)
        self.out_project = Conv(c1=final_channels, c2=channels, k=1, act=True)

    def _apply_axis_attention(self, x, axis):
        """é€šç”¨è½´æ³¨æ„åŠ›è®¡ç®—"""
        qkv = self.qkv[axis](x)
        query, key, value = torch.split(
            qkv, [1, self.middle_channels, self.middle_channels], dim=1
        )

        # æ˜Žç¡®æŒ‡å®šsoftmaxç»´åº¦
        dim = -1 if axis == "W" else -2
        context_scores = F.softmax(query, dim=dim)
        context_vector = (key * context_scores).sum(dim=dim, keepdim=True)
        # gate = F.tanh(self.alpha[axis] * value) # æ•ˆæžœä¸åŠsigmoid
        # gate = F.silu(value) # æ•ˆæžœæœ€å·®
        gate = F.sigmoid(value)
        # å°†å…¨å±€ä¸Šä¸‹æ–‡å‘é‡ä¹˜ä»¥æƒé‡ï¼Œå¹¶å¹¿æ’­æ³¨å…¥åˆ°ç‰¹å¾å›¾ä¸­
        out = x + gate * context_vector.expand_as(value)
        return out

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º [B, C, H, W]

        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º [B, C, H, W]
        """
        x_short = self.short_conv(x)
        x_main = self.main_conv(x)
        # å®½è½´æ³¨æ„åŠ›
        x_W = self._apply_axis_attention(x_main, "W")
        x_W_fused = self.conv_fusion["W"](x_W) + x_W
        # é«˜è½´æ³¨æ„åŠ›
        x_H = self._apply_axis_attention(x_W_fused, "H")
        x_H_fused = self.conv_fusion["H"](x_H) + x_H

        x_out = torch.cat([x_H_fused, x_short], dim=1)

        x_out = self.out_project(x_out)

        return x_out

```


æˆ‘ä»¬æ¥è¯¦ç»†åˆ†æž `DualAxisAggAttn` çš„å‚æ•°å¤æ‚åº¦ï¼ˆå³æƒé‡æ•°é‡ï¼‰ã€**å†…å­˜ç©ºé—´å¤æ‚åº¦** å’Œ  **æ—¶é—´å¤æ‚åº¦ï¼ˆæŽ¨ç†è®¡ç®—é‡ï¼‰** ï¼Œå‡è®¾è¾“å…¥å¼ é‡ `x` çš„å½¢çŠ¶ä¸º `[B, C, H, W]`ï¼Œè®¾ï¼š

* `B`ï¼šbatch size
* `C`ï¼šè¾“å…¥é€šé“æ•°
* `H, W`ï¼šç©ºé—´å°ºå¯¸
* `r`ï¼š`middle_ratio`ï¼Œé»˜è®¤ä¸º 0.5
* `C_m = C * r`ï¼šä¸­é—´ç‰¹å¾é€šé“æ•°

---

### ðŸ”§ ä¸€ã€å‚æ•°å¤æ‚åº¦

ä¸‹é¢åˆ†åˆ«ç»Ÿè®¡å„å±‚çš„å‚æ•°é‡ï¼š

#### 1. `main_conv` å’Œ `short_conv`

éƒ½æ˜¯ `1Ã—1` å·ç§¯ï¼Œè¾“å…¥è¾“å‡ºä¸º `C â†’ C_m`ï¼š

* æ¯ä¸ªå·ç§¯å‚æ•°æ•°ï¼š`C_m Ã— C Ã— 1 Ã— 1 = C_m * C`
* æœ‰ä¸¤ä¸ªè¿™æ ·çš„å·ç§¯ï¼Œæ€»å‚æ•°é‡ä¸ºï¼š`2 Ã— C_m Ã— C = 2CrC`

#### 2. `qkv` éƒ¨åˆ†

å¯¹ä¸¤ä¸ªè½´ï¼ˆW, Hï¼‰å„æœ‰ä¸€ä¸ªå·ç§¯å±‚ï¼š

* æ¯å±‚è¾“å‡ºé€šé“ä¸º `1 + 2*C_m`ï¼Œè¾“å…¥ä¸º `C_m`ï¼Œkernel size = 1

  æ‰€ä»¥æ¯å±‚å‚æ•°ä¸ºï¼š`(1 + 2C_m) Ã— C_m Ã— 1 Ã— 1 = (1 + 2C_m) * C_m`
* å…±æœ‰ä¸¤ä¸ªè½´ï¼Œå…±å‚æ•°ï¼š`2 Ã— (1 + 2C_m) * C_m`

#### 3. `light_ConvBlock`ï¼ˆä¸¤ä¸ªè½´å„ä¸€ä¸ªï¼‰

æ¯ä¸ª `light_ConvBlock` åŒ…å«ï¼š

* 1x1 Convï¼ˆC_m â†’ C_mï¼‰ï¼š`C_m Ã— C_m`
* 3x3 DWConvï¼ˆC_m â†’ C_mï¼Œgroups=C_mï¼‰ï¼š`3Ã—3Ã—C_m`
* 1x1 Convï¼ˆC_m â†’ C_mï¼‰ï¼š`C_m Ã— C_m`

æ€»ä¸ºï¼š`2*C_m*C_m + 9*C_m`

ä¸¤ä¸ªè½´æ€»å…±ï¼š`2 Ã— (2*C_m*C_m + 9*C_m)`

#### 4. `out_project`ï¼ˆ1Ã—1å·ç§¯ï¼‰ï¼šè¾“å…¥é€šé“ä¸º `2*C_m`ï¼Œè¾“å‡ºé€šé“ä¸º `C`ï¼š

* å‚æ•°é‡ä¸ºï¼š`C Ã— 2*C_m = 2CrC`

---

âœ…  **æ€»å‚æ•°å¤æ‚åº¦** ï¼ˆå¿½ç•¥BNä¸Žbiasï¼‰ä¸ºï¼š

$$
\text{Params} = 2CrC + 2(1 + 2CrC) + 4C_m^2 + 18C_m + 2CrC
$$

è¿‘ä¼¼åŒ–åŽä¸»å¯¼é¡¹æ˜¯ï¼š

$$


$$

$$
O\mathcal(C^2r^2)
$$

---

$$


$$

### ðŸ§  äºŒã€ç©ºé—´å¤æ‚åº¦ï¼ˆå†…å­˜ä½¿ç”¨ï¼‰

ç©ºé—´å¤æ‚åº¦è€ƒè™‘ä»¥ä¸‹å‡ éƒ¨åˆ†ï¼š

1. **ä¸­é—´ç‰¹å¾å›¾**
   * `x_main`ã€`x_short`ï¼šå¤§å° `[B, C_m, H, W]`
   * ä¸­é—´ `qkv` è¾“å‡ºï¼š[B, 1+2C_m, H, W]ï¼ˆå…±ä¸¤ä¸ªè½´ï¼‰
   * ä¸­é—´ `gate`ã€`context_vector`ï¼šå„ [B, C_m, H, W]

è¿™äº›éƒ½ä¸é‡å¤é‡Šæ”¾ï¼Œæ‰€ä»¥ç©ºé—´å¤æ‚åº¦å¤§çº¦æ˜¯ï¼š

$$
\mathcal{O}(B \cdot C_m \cdot H \cdot W)
$$

å› ä¸º `C_m = rC`ï¼Œæ‰€ä»¥ç©ºé—´å¤æ‚åº¦ä¸ºï¼š

$$


$$

$$
\mathcal{O}(B \cdot C \cdot H \cdot W)
$$


$$


$$

---

### â± ä¸‰ã€æ—¶é—´å¤æ‚åº¦ï¼ˆFLOPsï¼‰

ä¸»è¦è€ƒè™‘å·ç§¯å’Œæ³¨æ„åŠ›è®¡ç®—ï¼š

#### 1. `Conv1x1` å’Œ `DWConv`

* `1Ã—1` å·ç§¯ FLOPs: `2 Ã— H Ã— W Ã— in_channels Ã— out_channels`
* `DWConv 3Ã—3`: `2 Ã— H Ã— W Ã— 3 Ã— 3 Ã— C_m`

#### 2. `qkv` éƒ¨åˆ†ï¼š

* `Conv1x1`: `2 Ã— H Ã— W Ã— C_m Ã— (1 + 2C_m)`ï¼Œå…±ä¸¤ä¸ªè½´

#### 3. `æ³¨æ„åŠ›æ“ä½œ`

* Softmax over H/W axisï¼šä»£ä»·å°
* `key * context_scores`ï¼šç‚¹ä¹˜åŠ æƒæ±‚å’Œæ˜¯ï¼š`B Ã— C_m Ã— H Ã— W`

#### 4. æ€»ä½“ FLOPs

æ€» FLOPs ä¸»å¯¼é¡¹ä»ç„¶æ˜¯å·ç§¯ï¼Œå³ï¼š

* `light_ConvBlock` ä¸­ï¼šDWConvã€1x1Conv â†’ `â‰ˆ 4 Ã— H Ã— W Ã— C_m^2`
* `qkv` ä¸­çš„ 1x1 Conv â†’ `â‰ˆ 2 Ã— H Ã— W Ã— C_m^2`
* `out_project` â†’ `2 Ã— H Ã— W Ã— 2C_m Ã— C = 4 Ã— H Ã— W Ã— C_m Ã— C`

æ‰€ä»¥æ—¶é—´å¤æ‚åº¦çº¦ä¸ºï¼š

$$
\mathcal(H \cdot W \cdot C^2 \cdot r^2)
$$

---

$$


$$

### âœ… æ€»ç»“

| é¡¹ç›®                 | å¤æ‚åº¦                     | ä¸»å¯¼é¡¹è¯´æ˜Ž            |
| -------------------- | -------------------------- | --------------------- |
| **å‚æ•°å¤æ‚åº¦** | $\mathcal{O}(C^2 r^2)$   | å¤šä¸ª `1Ã—1`å·ç§¯ä¸ºä¸» |
| **ç©ºé—´å¤æ‚åº¦** | $\mathcal{O}(BCHW)$      | ä¸­é—´å¼ é‡ä¸Žè¾“å…¥åŒé˜¶    |
| **æ—¶é—´å¤æ‚åº¦** | $\mathcal{O}(HWC^2 r^2)$ | å·ç§¯è®¡ç®—ä¸ºä¸»          |

> å¦‚æžœ `r=0.5`ï¼Œé‚£ä¹ˆç›¸å¯¹æ™®é€š `Câ†’C` çš„ 1Ã—1 å·ç§¯å¤§çº¦è®¡ç®—é‡æ˜¯ 1/4ï½ž1/2 å€ï¼Œä»£ä»·å¯æŽ§ã€‚
>
